{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01morjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m505.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/r2a/lib/python3.8/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Using cached tokenizers-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = \"/home/ai2lab/work/big_models/llama3-llava-next-8b\"\n",
    "SYSTEM_PROMPT = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "with open('../../data/processed_data_v2/refcoco_data.json', 'r') as f:\n",
    "    all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/ade20k_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/paco_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/partimagenet_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grouped_data = {}\n",
    "for d in all_data:\n",
    "    img_grouped_data.setdefault(d['image_path'], []).append(d)\n",
    "    \n",
    "img_bboxs = {}\n",
    "for d in all_data:\n",
    "    if len(d['bboxes']) > 0:\n",
    "        img_bboxs.setdefault(d['image_path'], []).append(tuple(d['bboxes'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_grouped_data), len(img_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_group = list(img_grouped_data.values())[777]\n",
    "d = sample_group[10]\n",
    "len(sample_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['phrases'] for x in sample_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {\n",
    "    '1': 'single answer',\n",
    "    '1+': 'maybe multiple answers',\n",
    "    '0+': 'maybe no or multiple answers',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_str(bbox):\n",
    "    return f\"[{bbox[0]:03d},{bbox[1]:03d},{bbox[2]:03d},{bbox[3]:03d}]\"\n",
    "\n",
    "def point_to_str(point):\n",
    "    return f\"({point[0]:03d},{point[1]:03d})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PACKING = 5\n",
    "all_convs = []\n",
    "\n",
    "for img_path, sample_group in tqdm(img_grouped_data.items()):\n",
    "\n",
    "    sample_group_copy = deepcopy(sample_group)\n",
    "    random.shuffle(sample_group_copy)\n",
    "    \n",
    "    to_i_sample = 0\n",
    "    for _ in range(20):\n",
    "        if to_i_sample >= len(sample_group_copy):\n",
    "            break\n",
    "\n",
    "        img_conv = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        \n",
    "        for i_conv, i_sample in enumerate(range(to_i_sample, to_i_sample+MAX_PACKING)):\n",
    "            if i_sample >= len(sample_group_copy):\n",
    "                break\n",
    "            \n",
    "            ref_sample = sample_group_copy[i_sample]\n",
    "            \n",
    "            ref_conv = []\n",
    "            if isinstance(ref_sample['phrases'], list):\n",
    "                s_phrase = random.choice(ref_sample['phrases'])\n",
    "            else:\n",
    "                s_phrase = ref_sample['phrases']\n",
    "            \n",
    "            # print(s_phrase)\n",
    "            \n",
    "            # answer_counts = ref_sample['answer_counts']\n",
    "            # answer_counts_str = count_dict[answer_counts]\n",
    "            \n",
    "            bboxes = np.array(ref_sample['bboxes'])\n",
    "            points_and_labels = ref_sample['points_and_labels']\n",
    "            \n",
    "            answer_counts_str = '0+'\n",
    "            question_box = '<image>\\n' if i_conv == 0 else ''\n",
    "            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts_str}):\\n\"{s_phrase}\".'\n",
    "            if len(bboxes) == 0:\n",
    "                answer_box = 'No object found.'\n",
    "            else:\n",
    "                answer_box = ' '.join([bbox_to_str(x) for x in bboxes])\n",
    "\n",
    "            ref_conv.extend([\n",
    "                {\"role\": \"user\", \"content\": question_box},\n",
    "                {\"role\": \"assistant\", \"content\": f'\\n{answer_box}'}\n",
    "            ])\n",
    "            \n",
    "            bb_pnls = list(zip(bboxes, points_and_labels))\n",
    "            random.shuffle(bb_pnls)\n",
    "            for bbox, p_n_ls in bb_pnls:\n",
    "                n_sel_points = random.normalvariate(10, 4)\n",
    "                n_sel_points = int(max(1, min(20, n_sel_points)))\n",
    "                # print('n_sel_points', n_sel_points)\n",
    "                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)\n",
    "                \n",
    "                points_txt = ' '.join([point_to_str(x[:2]) for x in sampled_points_and_labels])\n",
    "                question_points = 'Check if the points listed below are located on the object with bounding box {}:\\n{}'.format(\n",
    "                    bbox_to_str(bbox), points_txt)\n",
    "                answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])\n",
    "                \n",
    "                ref_conv.extend([\n",
    "                    {\"role\": \"user\", \"content\": question_points},\n",
    "                    {\"role\": \"assistant\", \"content\": f'\\n{answer_points}'}\n",
    "                ])\n",
    "            \n",
    "            test_input_ids = tokenizer.apply_chat_template(img_conv + ref_conv, tokenize=True)\n",
    "            # print(len(test_input_ids))\n",
    "            if len(test_input_ids) > 1536:\n",
    "                # print('fulled! go next\\n')\n",
    "                break\n",
    "            else:\n",
    "                img_conv.extend(ref_conv)\n",
    "                to_i_sample = i_sample + 1\n",
    "            \n",
    "            \n",
    "        all_convs.append({\n",
    "            'image_path': img_path,\n",
    "            'conversation': img_conv\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./refcoco_convs_ep1.json', 'w') as f:\n",
    "    f.write(orjson.dumps(all_convs).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
