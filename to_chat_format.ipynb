{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = \"lmms-lab/llama3-llava-next-8b\"\n",
    "SYSTEM_PROMPT = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "with open('../../data/processed_data_v2/refcoco_data.json', 'r') as f:\n",
    "    all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/ade20k_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/paco_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())\n",
    "    \n",
    "# with open('../../data/processed_data_v2/partimagenet_ref_data.json', 'r') as f:\n",
    "#     all_data += orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grouped_data = {}\n",
    "for d in all_data:\n",
    "    img_grouped_data.setdefault(d['image_path'], []).append(d)\n",
    "    \n",
    "img_bboxs = {}\n",
    "for d in all_data:\n",
    "    if len(d['bboxes']) > 0:\n",
    "        img_bboxs.setdefault(d['image_path'], []).append(tuple(d['bboxes'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_grouped_data), len(img_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_group = list(img_grouped_data.values())[777]\n",
    "d = sample_group[10]\n",
    "len(sample_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['phrases'] for x in sample_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {\n",
    "    '1': 'single answer',\n",
    "    '1+': 'maybe multiple answers',\n",
    "    '0+': 'maybe no or multiple answers',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_str(bbox):\n",
    "    return f\"[{bbox[0]:03d},{bbox[1]:03d},{bbox[2]:03d},{bbox[3]:03d}]\"\n",
    "\n",
    "def point_to_str(point):\n",
    "    return f\"({point[0]:03d},{point[1]:03d})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PACKING = 5\n",
    "all_convs = []\n",
    "\n",
    "for img_path, sample_group in tqdm(img_grouped_data.items()):\n",
    "\n",
    "    sample_group_copy = deepcopy(sample_group)\n",
    "    random.shuffle(sample_group_copy)\n",
    "    \n",
    "    to_i_sample = 0\n",
    "    for _ in range(20):\n",
    "        if to_i_sample >= len(sample_group_copy):\n",
    "            break\n",
    "\n",
    "        img_conv = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        \n",
    "        for i_conv, i_sample in enumerate(range(to_i_sample, to_i_sample+MAX_PACKING)):\n",
    "            if i_sample >= len(sample_group_copy):\n",
    "                break\n",
    "            \n",
    "            ref_sample = sample_group_copy[i_sample]\n",
    "            \n",
    "            ref_conv = []\n",
    "            if isinstance(ref_sample['phrases'], list):\n",
    "                s_phrase = random.choice(ref_sample['phrases'])\n",
    "            else:\n",
    "                s_phrase = ref_sample['phrases']\n",
    "            \n",
    "            # print(s_phrase)\n",
    "            \n",
    "            # answer_counts = ref_sample['answer_counts']\n",
    "            # answer_counts_str = count_dict[answer_counts]\n",
    "            \n",
    "            bboxes = np.array(ref_sample['bboxes'])\n",
    "            points_and_labels = ref_sample['points_and_labels']\n",
    "            \n",
    "            answer_counts_str = '0+'\n",
    "            question_box = '<image>\\n' if i_conv == 0 else ''\n",
    "            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts_str}):\\n\"{s_phrase}\".'\n",
    "            if len(bboxes) == 0:\n",
    "                answer_box = 'No object found.'\n",
    "            else:\n",
    "                answer_box = ' '.join([bbox_to_str(x) for x in bboxes])\n",
    "\n",
    "            ref_conv.extend([\n",
    "                {\"role\": \"user\", \"content\": question_box},\n",
    "                {\"role\": \"assistant\", \"content\": f'\\n{answer_box}'}\n",
    "            ])\n",
    "            \n",
    "            bb_pnls = list(zip(bboxes, points_and_labels))\n",
    "            random.shuffle(bb_pnls)\n",
    "            for bbox, p_n_ls in bb_pnls:\n",
    "                n_sel_points = random.normalvariate(10, 4)\n",
    "                n_sel_points = int(max(1, min(20, n_sel_points)))\n",
    "                # print('n_sel_points', n_sel_points)\n",
    "                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)\n",
    "                \n",
    "                points_txt = ' '.join([point_to_str(x[:2]) for x in sampled_points_and_labels])\n",
    "                question_points = 'Check if the points listed below are located on the object with bounding box {}:\\n{}'.format(\n",
    "                    bbox_to_str(bbox), points_txt)\n",
    "                answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])\n",
    "                \n",
    "                ref_conv.extend([\n",
    "                    {\"role\": \"user\", \"content\": question_points},\n",
    "                    {\"role\": \"assistant\", \"content\": f'\\n{answer_points}'}\n",
    "                ])\n",
    "            \n",
    "            test_input_ids = tokenizer.apply_chat_template(img_conv + ref_conv, tokenize=True)\n",
    "            # print(len(test_input_ids))\n",
    "            if len(test_input_ids) > 1536:\n",
    "                # print('fulled! go next\\n')\n",
    "                break\n",
    "            else:\n",
    "                img_conv.extend(ref_conv)\n",
    "                to_i_sample = i_sample + 1\n",
    "            \n",
    "            \n",
    "        all_convs.append({\n",
    "            'image_path': img_path,\n",
    "            'conversation': img_conv\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./refcoco_convs_ep1.json', 'w') as f:\n",
    "    f.write(orjson.dumps(all_convs).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
