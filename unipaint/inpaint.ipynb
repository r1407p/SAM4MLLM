{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import kornia\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd()),\n",
    "sys.path.append('src/clip')\n",
    "sys.path.append('src/taming-transformers')\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.util import instantiate_from_config, load_model_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100  # num of fine-tuning iterations\n",
    "lr = 1e-5\n",
    "config= \"configs/stable-diffusion/v1-inference.yaml\"\n",
    "ckpt = \"ckpt/sd-v1-4-full-ema.ckpt\"  # path to SD checkpoint\n",
    "h = w = 512\n",
    "scale=8  # cfg scale\n",
    "ddim_steps= 50\n",
    "ddim_eta=0.0\n",
    "seed_everything(42)\n",
    "n_samples = 4\n",
    "out_path = \"outputs/\" \n",
    "gpu_id = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "config = OmegaConf.load(config)\n",
    "model = load_model_from_config(config, ckpt, device)\n",
    "sampler = DDIMSampler(model)\n",
    "params_to_be_optimized = list(model.model.parameters())\n",
    "optimizer = torch.optim.Adam(params_to_be_optimized, lr=lr)\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = lambda _x: torch.clamp(model.decode_first_stage(_x), min=-1, max=1).detach() # vae decode\n",
    "E = lambda _x: model.get_first_stage_encoding(model.encode_first_stage(_x))  # # vae encode\n",
    "img_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.unsqueeze(0) * 2. - 1)])\n",
    "mask_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x.unsqueeze(0) > 0).float())])\n",
    "\n",
    "# text encode\n",
    "def C(_txt, enable_emb_manager=False):\n",
    "    _txt = [_txt] if isinstance(_txt,str) else _txt\n",
    "    with torch.enable_grad() if enable_emb_manager else torch.no_grad(): # # disable grad flow unless we want textual inv\n",
    "        c = model.get_learned_conditioning(_txt, enable_emb_manager)\n",
    "        return c\n",
    "\n",
    "# save tensor as image file\n",
    "def tsave(tensor, save_path, **kwargs):\n",
    "    save_image(tensor, save_path, normalize=True, scale_each=True, value_range=(-1, 1), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read image and mask, and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"examples/dog.png\" \n",
    "mask_path = \"examples/dog-mask.png\" \n",
    "\n",
    "# read image and mask\n",
    "image = Image.open(image_path).convert('RGB').resize((h,w), Image.Resampling.BILINEAR)\n",
    "mask = Image.open(mask_path).convert('L').resize((h,w), Image.Resampling.BILINEAR)\n",
    "\n",
    "# x = img_transforms(image).repeat(n_samples, 1, 1, 1).to(device)\n",
    "x = img_transforms(image).to(device)\n",
    "# m = mask_transforms(mask).repeat(n_samples, 1, 1, 1).to(device)\n",
    "m = mask_transforms(mask).to(device)\n",
    "x_in = x * (1 - m)\n",
    "z_xm = E(x_in)\n",
    "z_m = F.interpolate(m, size=(h // 8, w // 8)) # latent mask\n",
    "z_m = kornia.morphology.dilation(z_m, torch.ones((3,3),device=device)) # dilate mask a little bit\n",
    "\n",
    "attn_mask = {}\n",
    "for attn_size in [64,32,16,8]:  # create attention masks for multi-scale layers in unet\n",
    "    attn_mask[str(attn_size**2)]= (F.interpolate(m, (attn_size,attn_size), mode='bilinear'))[0,0,...]\n",
    "\n",
    "uc = C(\"\")  # null-text emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "pbar = tqdm(range(num_iter), desc='Fine-tune the model')\n",
    "for i in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    noise = torch.randn_like(z_xm)\n",
    "    t_emb = torch.randint(model.num_timesteps, (1,), device=device)\n",
    "    z_t = model.q_sample(z_xm, t_emb, noise=noise)\n",
    "    pred_noise = model.apply_model(z_t, t_emb, uc)\n",
    "\n",
    "    loss_noise = F.mse_loss(pred_noise * (1 - z_m), noise * (1 - z_m))\n",
    "    loss = loss_noise\n",
    "\n",
    "    losses_dict = {\"loss\": loss}\n",
    "    pbar.set_postfix({k: v.item() for k,v in losses_dict.items()})\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference - Unconditional inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad(), torch.autocast(device.type):\n",
    "    # uncond inpainting\n",
    "    tmp, _ = sampler.sample(S=ddim_steps, batch_size=n_samples, shape=[4, h // 8, w // 8],\n",
    "                            conditioning=uc.repeat(n_samples,1,1), \n",
    "                            unconditional_conditioning=uc.repeat(n_samples,1,1),\n",
    "                            blend_interval=[0, 1], \n",
    "                            x0=z_xm.repeat(n_samples,1,1,1), \n",
    "                            mask=z_m.repeat(n_samples,1,1,1), \n",
    "                            attn_mask=attn_mask,\n",
    "                            x_T=None, \n",
    "                            unconditional_guidance_scale=scale, \n",
    "                            eta=ddim_eta,\n",
    "                            verbose=False)\n",
    "\n",
    "    tsave(D(tmp), os.path.join(out_path, f'Uncond.jpg'), nrow=n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference - Text inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a vase of flower'\n",
    "\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device.type):\n",
    "    tmp, _ = sampler.sample(S=ddim_steps, batch_size=n_samples, shape=[4, h // 8, w // 8],\n",
    "                        conditioning=C(prompt).repeat(n_samples,1,1), \n",
    "                        unconditional_conditioning=uc.repeat(n_samples,1,1),\n",
    "                        blend_interval=[0, 1], \n",
    "                        x0=z_xm.repeat(n_samples,1,1,1), \n",
    "                        mask=z_m.repeat(n_samples,1,1,1), \n",
    "                        attn_mask=attn_mask,\n",
    "                        x_T=None, \n",
    "                        unconditional_guidance_scale=scale, \n",
    "                        eta=ddim_eta,\n",
    "                        verbose=False)\n",
    "\n",
    "    tsave(D(tmp), os.path.join(out_path, f'Text-{prompt}.jpg'), nrow=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference - Stroke inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stroke image\n",
    "stroke_path = \"examples/wn-stroke-blue.png\" \n",
    "stroke = Image.open(stroke_path).convert('RGB').resize((h,w), Image.Resampling.BILINEAR)\n",
    "x_stroke = img_transforms(stroke).to(device)\n",
    "x_stroke_mask = (torch.mean(x_stroke,dim=1,keepdim=True) > -1).float()\n",
    "z_stroke = E(x_stroke)\n",
    "z_stroke_mask = F.interpolate(x_stroke_mask, size=(h // 8, w // 8))\n",
    "\n",
    "\n",
    "prompt = 'a toy bear'\n",
    "tau = 0.55 # stroke blending timestep\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(), torch.autocast(device.type):\n",
    "    tmp, _ = sampler.sample(S=ddim_steps, batch_size=n_samples, shape=[4, h // 8, w // 8],\n",
    "                        conditioning={'t': [[0, tau], [tau, 1]], 'c': [C(prompt).repeat(n_samples,1,1), uc.repeat(n_samples,1,1)]}, \n",
    "                        unconditional_conditioning=uc.repeat(n_samples,1,1),\n",
    "                        blend_interval=[[0, 1], [tau, tau + 0.02]], \n",
    "                        x0=[z_xm.repeat(n_samples,1,1,1), z_stroke.repeat(n_samples,1,1,1)], \n",
    "                        mask=[z_m.repeat(n_samples,1,1,1), 1 - z_stroke_mask.repeat(n_samples,1,1,1)], \n",
    "                        attn_mask=attn_mask,\n",
    "                        x_T=None, \n",
    "                        unconditional_guidance_scale=scale, \n",
    "                        eta=ddim_eta,\n",
    "                        verbose=False)\n",
    "\n",
    "    tsave(D(tmp), os.path.join(out_path, f'Stroke-{prompt}.jpg'), nrow=n_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
